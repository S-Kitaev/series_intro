# Series_intro
Тестовое задание на стажировку от VK - сделать и имплементировать подход по обнаружению времени заставки сериала в видеоряде 

Имплементацию задания решил выполнить в виде обучения нейросети YOLO втраивания ее в алгоритм поиска заставки и реализации поиска заставки при помощи web-приложения, для удобного использования программы.
Работа состоит из двух частей:

1. Все что связано с машинным обучением выполнялось на Kaggle, используемый стэк: YOLO, numpy, pandas, json, matplotlib, cv2.
2. Реализация имплементация решения - веб-приложение, выполнялась локально, стэк: fastapi, docker, cv2, YOLO.

## 1. Описание идеи работы алгоритма + нейросети для поиска времени заставки

Универсального подхода по работе с видео на данный момент не существует, большинство доступных решений (kaggle, hugging face) - это работа с изображениями или работа с видео но покадрово, то необходимо придумать свой способ реализации задачи поиска времени начала и окончания заставки.
Видео в общем смысле состоит из 3х составляющих - кадры, временное распределение кадров, звук. Для полноценного решения задачи определения времени заставки необходимо использовать все 3 данных параметра.
Из рассмотренных вариантов, изученных открытых решений и опыта решения задач связанных с видео можно рассмотреть несколько подходов:
- Первый вариант - рассмотреть видео - как 3д матрицу, в общем случае 4д матрицу (терзор) - 1 ось - цветовые каналы, 2 оси ширина и высота кадра, 1 ось - набор кадров. Тогда после использовании 4д свертки можно получить 3х мерное изображение, с которым уже можно работать далее при помощи нейросети, но есть несколько ограничений: сложность написания такой сети в короткие сроки и малый размер датасета - всего 125 (123 после удаления выбросов) видео, по такому количеству элементов для обучения сложно получить хороший результат
- Второй вариант (основа моей стратегии) - это разметить датасет покадрово на 2 класса - кадр заставки/кадр не заставки и решать задачу бинарной классификации кадров. Так как подходящих кадров в каждом видео будет в среднем mean(время заставки) * mean(fps) * 85 * 1/4 (берем каждый 4й кадр) = 6.6 тыс кадров заставок + столько же кадров не заставок добавить, в итоге получится 13.2 тыс картинок (кадров) если сбалансировать классы, или даже более если сделать классы несбалансироваными, что является достаточным для обучения нейросети датасетом. Использовать я планирую нейросеть YOLOv11cls, предобученной версии, поэтому на выходе можно получать вероятность принадлежности кадра к заставке. После того, как такая нейросеть пройдется по всем кадрам видео будет участок видео, где кадры идущие подряд принадлежат классу заставка, значит заставка будет идти именно в этом месте. Следовательно можно получить конкретный номер кадра - когда начинается и заканчивается заставка, а значит и конкретные временные метки.
- Третий подход - это файнтюнинг второго подхода и мультимодальная стратегия - например вместо одного из цветового канала картинки можно вставить фильтр Cобеля, так как в заставке присутствует контрастный текст, что позволит модели выделять такие кадры и к получившемуся результату добавить вторую нейросеть, которая будет анализировать только звуковой канал и выдавать метки времени начала и конца заставки исходя из звука, потом после усреднения результата звуковой модели и модели картиночной (с некоторыми весовыми коэффициентами) можно будет получить результат.
Базово мною был выполнен второй вариант, в дальнейшем к нему можно будет добавить третий вариант

Работа алгоритма:

Алгоритм в бэке будет принимать видео, затем ресайзит его до размера 224 на 224 пикселя, как картинки на которых обучалась модель YOLO, берет каждый 8й кадр (для ускорения работы), и прогоняет его через модель, которая выдает 1 если кадр это заставка и 0 если кадр не заставка, на выходе получается массив [0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,] - где расстояние от первой 1 до последней 1 это время заставки, дополнительно добавил механизм сглаживания чтобы убрать выбросы. Конвертируя информацю в обратной последовательности - индекс кадра умножаем на 8, затем переводим в секунды - получаем точное время начала а также окончания заставки. Вкратце все представлено на картинке ниже

![image](https://github.com/user-attachments/assets/b9ab00ca-2fa2-4cf2-8787-e6af69841c9b)

## 2. Описание процесса написания кода.

1. Сначала мною на kaggle был загружены файлы в приватный датасет доступный по ссылке - https://kaggle.com/datasets/137540251c393dc240a01dd5f7b828ef5b67d20883637ac5b8503cfea7484b81
2. Затем я провел аналитику датасета в отдельном блокноте, блокнот загружен ml_research/dataset_research.ipynb
3. Далее я начал подготовку данных к обучению нейросети, подготовленные данные также загрузил в датасет, после начал обучение нейросети. Блокнот загружен ml_research/model_research.ipynb точность модели бинарной классификации по метрике accuracy - ```0.932```
4. Далее готовая модель была встроена в бэкенд - модель надится по пути backend/models/model.pt

Ниже представлены графики loss/epoch и accuracy/epoch

![image](https://github.com/user-attachments/assets/9335f281-70b4-4170-b89a-143d58ae86fc)

5. Также написал файлы для запуска приложения в docker, но проверить работоспособность не успел. 

## 3. Запуск веб приложения

1. Скачайте архив с программой или склонируйте репозиторий
2. Откройте терминал/командную строку в папке с программой
3. Выполните команду для установки компонентов:
```bash
pip install -r requirements.txt
```
4. Запустите веб-интерфейс в терминале из папки с проектом командой:
```bash
uvicorn main:app --reload --port 8000
```
5. Откройте браузер и в верхней строке введите локальный адрес приложения:
```bash
http://127.0.0.1:8000
```
Откроется данная страница:

![image](https://github.com/user-attachments/assets/dc45a262-e0ff-45f1-bf2b-c874a649b575)

6. Загрузите видео и нажмите кнопку ```Найти заставку```

![image](https://github.com/user-attachments/assets/d4b0b35d-bf03-4ca4-a678-f7ae88a97f59)

7. Далее видео будет обрабатываться, на выходе снизу на странице будет выведено время начала и время окончания заставки. Без ГПУ это занимает приблизительно 3 минуты.

![image](https://github.com/user-attachments/assets/ab8130ab-a4bb-4af4-99ae-bf94481c66f0)


Мои контакты:
почта: kitaev.ste@yandex.ru
tg: @Mathboyy

С нетерпением жду обратной связи по данному проекту. Хотелось бы узнать что можно было сделать лучше.

